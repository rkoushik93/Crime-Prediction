{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a. I've used another csv file for copying the entire data and to add the new field \"highCrime\" to avoid complications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TrueCount = ', 1250)\n",
      "('FalseCount = ', 743)\n",
      "('TotalCount = ', 1993)\n",
      "0\n",
      "('Percentage of (highCrime=true) = ', 0, '%')\n",
      "('Percentage of (highCrime=false) = ', 0.0, '%')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "reader=csv.reader(open('communities-crime-clean.csv','rU'))\n",
    "writer=csv.writer(open('outputSample.csv','wb'))\n",
    "i=0\n",
    "for row in reader:\n",
    "    if(i==0):\n",
    "        row.append(\"highCrime\")\n",
    "        i=1\n",
    "    if(row[103]!='ViolentCrimesPerPop'):\n",
    "        if(float(row[103])>0.1):\n",
    "            row.append(\"true\")\n",
    "        else:\n",
    "            row.append(\"false\")\n",
    "    writer.writerow(row)\n",
    "\n",
    "totalCount=0\n",
    "trueCount=0\n",
    "falseCount=0\n",
    "\n",
    "tempRead=csv.reader(open('outputSample.csv','r'))\n",
    "\n",
    "for row0 in tempRead:\n",
    "    if(row0[104]=='true'):\n",
    "        trueCount=trueCount+1\n",
    "        totalCount=totalCount+1\n",
    "    if(row0[104]=='false'):\n",
    "        falseCount=falseCount+1\n",
    "        totalCount=totalCount+1\n",
    "\n",
    "truePercent=100\n",
    "falsePercent=100\n",
    "truePercent=(trueCount/totalCount)\n",
    "falsePercent=100*float(falseCount/totalCount)\n",
    "print(\"TrueCount = \",trueCount)\n",
    "print(\"FalseCount = \",falseCount)\n",
    "print(\"TotalCount = \",totalCount)\n",
    "print(truePercent)\n",
    "print(\"Percentage of (highCrime=true) = \",truePercent,\"%\")\n",
    "print(\"Percentage of (highCrime=false) = \",falsePercent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b. Using the DecisionTreeClassifier module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:  1993\n",
      "Dataset Shape::  (1993, 105)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=598, splitter='best')\n",
      "Accuracy =  77.2575250836 %\n",
      "Precision =  82.8205128205 %\n",
      "Recall =  82.3979591837 %\n",
      "Feature ranking:\n",
      "1. feature 44 (0.352464)\n",
      "2. feature 3 (0.084919)\n",
      "3. feature 5 (0.057541)\n",
      "4. feature 71 (0.020375)\n",
      "5. feature 32 (0.020074)\n",
      "6. feature 47 (0.019796)\n",
      "7. feature 78 (0.019442)\n",
      "8. feature 33 (0.019381)\n",
      "9. feature 46 (0.018322)\n",
      "10. feature 22 (0.017254)\n",
      "11. feature 45 (0.015575)\n",
      "12. feature 38 (0.013152)\n",
      "13. feature 12 (0.011717)\n",
      "14. feature 25 (0.011295)\n",
      "15. feature 23 (0.010816)\n",
      "16. feature 61 (0.010635)\n",
      "17. feature 0 (0.010594)\n",
      "18. feature 39 (0.009957)\n",
      "19. feature 65 (0.009119)\n",
      "20. feature 72 (0.009055)\n",
      "21. feature 85 (0.008782)\n",
      "22. feature 68 (0.008301)\n",
      "23. feature 83 (0.008175)\n",
      "24. feature 30 (0.008171)\n",
      "25. feature 24 (0.008148)\n",
      "26. feature 21 (0.008135)\n",
      "27. feature 60 (0.007858)\n",
      "28. feature 54 (0.007707)\n",
      "29. feature 2 (0.007355)\n",
      "30. feature 88 (0.007299)\n",
      "31. feature 95 (0.007266)\n",
      "32. feature 43 (0.007209)\n",
      "33. feature 26 (0.006890)\n",
      "34. feature 92 (0.006652)\n",
      "35. feature 91 (0.006596)\n",
      "36. feature 64 (0.006459)\n",
      "37. feature 18 (0.006407)\n",
      "38. feature 13 (0.006006)\n",
      "39. feature 40 (0.005823)\n",
      "40. feature 48 (0.005588)\n",
      "41. feature 17 (0.005547)\n",
      "42. feature 77 (0.005429)\n",
      "43. feature 16 (0.005324)\n",
      "44. feature 7 (0.005324)\n",
      "45. feature 97 (0.005124)\n",
      "46. feature 86 (0.005051)\n",
      "47. feature 53 (0.004885)\n",
      "48. feature 66 (0.004866)\n",
      "49. feature 28 (0.004738)\n",
      "50. feature 81 (0.004711)\n",
      "51. feature 96 (0.004434)\n",
      "52. feature 52 (0.004325)\n",
      "53. feature 34 (0.004325)\n",
      "54. feature 36 (0.004239)\n",
      "55. feature 79 (0.004235)\n",
      "56. feature 93 (0.004205)\n",
      "57. feature 9 (0.004037)\n",
      "58. feature 11 (0.003495)\n",
      "59. feature 99 (0.003407)\n",
      "60. feature 55 (0.003315)\n",
      "61. feature 8 (0.003206)\n",
      "62. feature 37 (0.002826)\n",
      "63. feature 4 (0.002691)\n",
      "64. feature 6 (0.002473)\n",
      "65. feature 74 (0.002422)\n",
      "66. feature 87 (0.002422)\n",
      "67. feature 75 (0.002271)\n",
      "68. feature 57 (0.002271)\n",
      "69. feature 31 (0.001693)\n",
      "70. feature 50 (0.000401)\n",
      "71. feature 82 (0.000000)\n",
      "72. feature 15 (0.000000)\n",
      "73. feature 14 (0.000000)\n",
      "74. feature 76 (0.000000)\n",
      "75. feature 80 (0.000000)\n",
      "76. feature 41 (0.000000)\n",
      "77. feature 10 (0.000000)\n",
      "78. feature 84 (0.000000)\n",
      "79. feature 90 (0.000000)\n",
      "80. feature 94 (0.000000)\n",
      "81. feature 1 (0.000000)\n",
      "82. feature 89 (0.000000)\n",
      "83. feature 20 (0.000000)\n",
      "84. feature 73 (0.000000)\n",
      "85. feature 27 (0.000000)\n",
      "86. feature 98 (0.000000)\n",
      "87. feature 35 (0.000000)\n",
      "88. feature 51 (0.000000)\n",
      "89. feature 56 (0.000000)\n",
      "90. feature 29 (0.000000)\n",
      "91. feature 58 (0.000000)\n",
      "92. feature 59 (0.000000)\n",
      "93. feature 19 (0.000000)\n",
      "94. feature 62 (0.000000)\n",
      "95. feature 63 (0.000000)\n",
      "96. feature 67 (0.000000)\n",
      "97. feature 69 (0.000000)\n",
      "98. feature 70 (0.000000)\n",
      "99. feature 42 (0.000000)\n",
      "100. feature 49 (0.000000)\n",
      "Most important feature is : feature  44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import tree\n",
    "\n",
    "data=pd.read_csv('C:/CrimePredictionData/outputSample.csv',header=0)\n",
    "\n",
    "print (\"Dataset Length: \", len(data))\n",
    "print (\"Dataset Shape:: \", data.shape)\n",
    "\n",
    "Xvalues=data.values[:,3:103]\n",
    "Yvalues=data.values[:,104]\n",
    "\n",
    "for i in range(len(Yvalues)):\n",
    "    Yvalues[i]=str(Yvalues[i])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xvalues,Yvalues,test_size = 0.3)\n",
    "\n",
    "dTreeClass = DecisionTreeClassifier(criterion = \"gini\", random_state = 598)\n",
    "print(dTreeClass)\n",
    "\n",
    "#print(Y_train)\n",
    "Y_training=list()\n",
    "for val in Y_train:\n",
    "    Y_training.append(val)\n",
    "\n",
    "#X_train=np.array(X_train).reshape(-1,1)\n",
    "#Y_training=Y_training.reshape(-1,1)\n",
    "y_training=np.asarray(Y_training)\n",
    "y_training=y_training.reshape(-1,1)\n",
    "dTreeClass=dTreeClass.fit(X_train,y_training)\n",
    "\n",
    "#X_test=np.array(X_test).reshape((-1,1))\n",
    "y_pred=dTreeClass.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = \",accuracy_score(Y_test,y_pred)*100,\"%\")\n",
    "print(\"Precision = \",precision_score(Y_test,y_pred,pos_label=\"True\")*100,\"%\")\n",
    "print(\"Recall = \",recall_score(Y_test,y_pred,pos_label=\"True\")*100,\"%\")\n",
    "importances = dTreeClass.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#print(\"Most important feature = feature \",importances[0])\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(Xvalues.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "print(\"Most important feature is : feature \",indices[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b.\n",
    "(i) Accuracy =  77.7591973244 %\n",
    "Precision =  83.9050131926 %\n",
    "Recall =  81.5384615385 %\n",
    "\n",
    "(ii) The most important feature used for classifying is PctKids2Par, which is the percentage of kids in family housing with 2 parents. This seems pretty logical as the percentage of kids increases in a family, the likelihood of crimes can go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c. Applying cross_val_score to do 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy =  0.719479899497\n",
      "Mean Precision =  0.781190893925\n",
      "Mean Recall =  0.7688\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "crossValScoreAccuracy=cross_val_score(dTreeClass, Xvalues, Yvalues,cv=10)\n",
    "print(\"Mean Accuracy = \",np.mean(crossValScoreAccuracy))\n",
    "\n",
    "precisionScorer = make_scorer(precision_score, pos_label='True')\n",
    "crossValScorePrecision=cross_val_score(dTreeClass,Xvalues,Yvalues,cv=10,scoring=precisionScorer)\n",
    "print(\"Mean Precision = \",np.mean(crossValScorePrecision))\n",
    "\n",
    "recallScorer=make_scorer(recall_score,pos_label='True')\n",
    "crossValScoreRecall=cross_val_score(dTreeClass,Xvalues,Yvalues,cv=10,scoring=recallScorer)\n",
    "print(\"Mean Recall = \",np.mean(crossValScoreRecall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c.\n",
    "(i) The metrics of the 10-fold cross-validation are :\n",
    "Mean Accuracy =  0.725979899497\n",
    "Mean Precision =  0.788705794886\n",
    "Mean Recall =  0.7712\n",
    "\n",
    "(ii) The difference between these values and the previous results is due to the sample taken for computation. In the previous case, we took 30% data for testing. But in this case, we are taking 10-folds data and computing the metrics for each fold and then we take the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. Using GaussianNB to learn a Naive Bayes classifier to predict highCrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes mean accuracy =  0.761608040201\n",
      "Naive Bayes mean precision =  0.911799814828\n",
      "Naive Bayes mean recall =  0.692\n",
      "Features and the differences : \n",
      "Feature  44  difference =  0.8097484282277082\n",
      "Feature  43  difference =  0.7455448082166711\n",
      "Feature  3  difference =  0.735229953123246\n",
      "Feature  50  difference =  0.7092610512005777\n",
      "Feature  40  difference =  0.6939780883033155\n",
      "Feature  41  difference =  0.6746446092224765\n",
      "Feature  45  difference =  0.665008569884773\n",
      "Feature  15  difference =  0.6610764326654875\n",
      "Feature  46  difference =  0.6429494481747667\n",
      "Feature  38  difference =  0.6168636592230385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nBayesClass=GaussianNB()\n",
    "nBayesClass.fit(X_train,Y_train)\n",
    "GaussianNB(priors=None)\n",
    "y_predBayes=nBayesClass.predict(X_test)\n",
    "#print(\"Predicted values : \",y_predBayes)\n",
    "\n",
    "nbAccuracy=cross_val_score(nBayesClass,Xvalues,Yvalues,cv=10)\n",
    "print(\"Naive Bayes mean accuracy = \",np.mean(nbAccuracy))\n",
    "\n",
    "nbPrecision=cross_val_score(nBayesClass,Xvalues,Yvalues,cv=10,scoring=precisionScorer)\n",
    "print(\"Naive Bayes mean precision = \",np.mean(nbPrecision))\n",
    "\n",
    "nbRecall=cross_val_score(nBayesClass,Xvalues,Yvalues,cv=10,scoring=recallScorer)\n",
    "print(\"Naive Bayes mean recall = \",np.mean(nbRecall))\n",
    "\n",
    "normAbsList=[]\n",
    "for i in range(Xvalues.shape[1]):\n",
    "    sumTrue=0.0\n",
    "    sumFalse=0.0\n",
    "    meanTrue=0.0\n",
    "    meanFalse=0.0\n",
    "    itr=0\n",
    "    countTrue=0\n",
    "    countFalse=0\n",
    "    for val in Xvalues[:,i]:\n",
    "        if(str(Yvalues.item(itr))=='True'):\n",
    "            sumTrue=sumTrue+val\n",
    "            countTrue=countTrue+1\n",
    "            itr=itr+1\n",
    "        else:\n",
    "            sumFalse=sumFalse+val\n",
    "            countFalse=countFalse+1\n",
    "            itr=itr+1\n",
    "    meanTrue=sumTrue/countTrue\n",
    "    meanFalse=sumFalse/countFalse\n",
    "    itr=0\n",
    "    sumSquareTrue=0.0\n",
    "    sumSquareFalse=0.0\n",
    "    for val1 in Xvalues[:,i]:\n",
    "        if(str(Yvalues.item(itr))=='True'):\n",
    "            sumSquareTrue=sumSquareTrue+((val1-meanTrue)*(val1-meanTrue))\n",
    "            itr=itr+1\n",
    "        else:\n",
    "            sumSquareFalse=sumSquareFalse+((val1-meanFalse)*(val1-meanFalse))\n",
    "            itr=itr+1\n",
    "    sdTrue=(sumSquareTrue/countTrue)**(1/2.0)\n",
    "    sdFalse=(sumSquareFalse/countFalse)**(1/2.0)\n",
    "    normAbsDif=(abs(meanTrue-meanFalse))/(sdTrue+sdFalse)\n",
    "    normAbsList.append(normAbsDif)\n",
    "    sortedIndices=np.argsort(normAbsList)[::-1]\n",
    "\n",
    "print(\"Features and the differences : \")\n",
    "for x in range(10):\n",
    "    print(\"Feature \",sortedIndices[x],\" difference = \",normAbsList[sortedIndices[x]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a.\n",
    "(i) The metrics of the 10-fold cross-validation are,\n",
    "    Naive Bayes mean accuracy =  0.761608040201\n",
    "    Naive Bayes mean precision =  0.911799814828\n",
    "    Naive Bayes mean recall =  0.692\n",
    "\n",
    "(ii) The 10 most predictive features are :\n",
    "    PctKids2Par, PctFam2Par, population, PctIlleg, FemalePctDiv, TotalPctDiv, PctYoungKids2Par, pctWInvInc, PctTeen2Par and MalePctDivorce\n",
    "    The larger the difference, the more predictive the feature is. This is because as the difference becomes large, there's a huge disparity between the values that correspond to the True class and False class. The class label can be easily determined as the difference gets large.\n",
    "\n",
    "(iii) These results are slightly different from that obtained from the Decision Tree. This is because of the difference in how both the models learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b. Implementing LinearSVC :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy =  0.796233668342\n",
      "Linear SVC precision =  0.845404856531\n",
      "Linear SVC recall =  0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.00230082,  0.64937299,  0.66921797,  1.50021955,  0.03286906,\n",
       "         0.77030062,  0.98827289,  0.75968963,  0.01963456,  0.890573  ,\n",
       "         0.03139657,  0.13348688,  0.39527268,  0.33962798,  0.20031268,\n",
       "         1.88848756,  0.09050429,  0.25781507,  0.07894048,  0.41782071,\n",
       "         0.28464109,  0.1731376 ,  0.05577263,  0.05690586,  0.10881711,\n",
       "         0.39363079,  0.18841948,  1.05154795,  0.32272802,  0.21534113,\n",
       "         0.0918559 ,  0.54218142,  0.22415196,  0.43352192,  0.1550712 ,\n",
       "         0.46510799,  0.00259216,  1.01467415,  1.06569569,  0.66952685,\n",
       "         0.20033822,  0.19165646,  0.1824921 ,  0.86980893,  1.19032855,\n",
       "         0.20274708,  0.37962831,  0.04120569,  0.08783801,  0.64728588,\n",
       "         0.04921386,  0.42902907,  0.31035694,  0.41097297,  0.21505436,\n",
       "         0.45430688,  0.26380772,  0.57995721,  0.02945877,  0.7137713 ,\n",
       "         0.49697695,  0.10324896,  0.36563778,  0.95739292,  1.75512634,\n",
       "         0.04692473,  0.27352158,  0.19317892,  0.79477812,  0.63205249,\n",
       "         0.00967735,  0.93651316,  0.19907402,  0.40415495,  0.02859563,\n",
       "         0.25065868,  0.14256923,  0.49060018,  0.36742032,  0.37838814,\n",
       "         0.02221948,  0.18969104,  0.70370062,  0.1492585 ,  1.06688432,\n",
       "         0.42279313,  0.33332305,  0.00345457,  0.29167565,  0.02957887,\n",
       "         1.01915443,  0.34077322,  0.31439547,  0.02065401,  0.28858598,\n",
       "         0.3732214 ,  0.79416807,  0.03235825,  0.14111386,  0.11611531]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "linearSvcModel=svm.LinearSVC()\n",
    "linearSvcModel.fit(Xvalues,Yvalues)\n",
    "svm.LinearSVC()\n",
    "\n",
    "svcAccuracy=cross_val_score(linearSvcModel,Xvalues,Yvalues,cv=10)\n",
    "print(\"Linear SVC accuracy = \",np.mean(svcAccuracy))\n",
    "\n",
    "svcPrecision=cross_val_score(linearSvcModel,Xvalues,Yvalues,cv=10,scoring=precisionScorer)\n",
    "print(\"Linear SVC precision = \",np.mean(svcPrecision))\n",
    "\n",
    "svcRecall=cross_val_score(linearSvcModel,Xvalues,Yvalues,cv=10,scoring=recallScorer)\n",
    "print(\"Linear SVC recall = \",np.mean(svcRecall))\n",
    "\n",
    "absFeatureWts=abs(linearSvcModel.coef_)\n",
    "index=np.argsort(absFeatureWts)[::-1]\n",
    "\n",
    "absFeatureWts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b.\n",
    "(i) The metrics from 10-fold cross-validation are:\n",
    "Linear SVC accuracy =  0.796233668342\n",
    "Linear SVC precision =  0.845404856531\n",
    "Linear SVC recall =  0.8344\n",
    "\n",
    "(ii) The 10 most predictive features are:\n",
    "\n",
    "pctWInvInc, PersPerOccupHous, racePctWhite, PctKids2Par, RentHighQ, MalePctDivorce, NumUnderPov, NumStreet, PctOccupMgmtProf, population\n",
    "\n",
    "(iii) These results are slightly different from the decision tree as the methods involved in learning by both the models are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a. Implementing LinearRegression to predict the crime rate per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MSE of 10-fold cross-validation =  -0.0200939693044\n",
      "Estimated MSE of training set =  -1.51827513156e-30\n",
      "Feature  64 's importance =  3.31503009227\n",
      "Feature  73 's importance =  2.96604077521\n",
      "Feature  38 's importance =  2.39468803667\n",
      "Feature  58 's importance =  2.25913430374\n",
      "Feature  85 's importance =  1.94752839434\n",
      "Feature  19 's importance =  1.5057912585\n",
      "Feature  33 's importance =  1.29988149452\n",
      "Feature  39 's importance =  1.18653263332\n",
      "Feature  68 's importance =  1.12202855573\n",
      "Feature  80 's importance =  1.1143315441\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "YregVal=data.values[:,103]\n",
    "\n",
    "linReg=linear_model.LinearRegression()\n",
    "linReg.fit(Xvalues,YregVal)\n",
    "linear_model.LinearRegression()\n",
    "\n",
    "YregPred=linReg.predict(Xvalues)\n",
    "mse=cross_val_score(linReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold cross-validation = \",np.mean(mse))\n",
    "\n",
    "mseEntire=cross_val_score(linReg,Xvalues,YregPred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training set = \",np.mean(mseEntire))\n",
    "\n",
    "meanCoeff=np.mean(linReg.coef_)\n",
    "coeffList=[]\n",
    "for val in linReg.coef_:\n",
    "    coeffList.append((val-meanCoeff)/np.std(linReg.coef_))\n",
    "    \n",
    "coefIndex=np.argsort(coeffList)[::-1]\n",
    "for i in range(10):\n",
    "    print(\"Feature \",coefIndex[i],\"'s importance = \",coeffList[coefIndex[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a.\n",
    "\n",
    "(i) Estimated MSE of 10-fold cross-validation =  -0.0200939693044\n",
    "\n",
    "(ii) Estimated MSE of training set =  -1.47277662963e-30\n",
    "\n",
    "(iii) The 10 most important features are :\n",
    "        PersPerOccupHouse, PctHousOwnOcc, MalePctDivorce, PctRecImmig8, MedRent, medFamInc, PctEmploy, MalePctNevMarr, PctPersDenseHous, OwnOccMedVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b. Using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MSE of 10-fold CV (alpha=10) =  -0.0200342497979\n",
      "Estimated MSE of training data (alpha=10) =  -0.000152996873221\n",
      "\n",
      "Estimated MSE of 10-fold CV (alpha=1) =  -0.0197950213482\n",
      "Estimated MSE of training data (alpha=1) =  -8.73728120217e-05\n",
      "\n",
      "Estimated MSE of 10-fold CV (alpha=0.1) =  -0.01991972494\n",
      "Estimated MSE of training data (alpha=0.1) =  -1.63387410814e-05\n",
      "\n",
      "Estimated MSE of 10-fold CV (alpha=0.01) =  -0.0200559556275\n",
      "Estimated MSE of training data (alpha=0.01) =  -1.31270726501e-06\n",
      "\n",
      "Estimated MSE of 10-fold CV (alpha=0.001) =  -0.020089527318\n",
      "Estimated MSE of training data (alpha=0.001) =  -2.31024123202e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ridReg=linear_model.Ridge(alpha=10)\n",
    "ridReg.fit(Xvalues,YregVal)\n",
    "yRidgePred=ridReg.predict(Xvalues)\n",
    "mse=cross_val_score(ridReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold CV (alpha=10) = \",np.mean(mse))\n",
    "mse=cross_val_score(ridReg,Xvalues,yRidgePred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training data (alpha=10) = \",np.mean(mse))\n",
    "print()\n",
    "\n",
    "ridReg=linear_model.Ridge(alpha=1)\n",
    "ridReg.fit(Xvalues,YregVal)\n",
    "yRidgePred=ridReg.predict(Xvalues)\n",
    "mse=cross_val_score(ridReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold CV (alpha=1) = \",np.mean(mse))\n",
    "mse=cross_val_score(ridReg,Xvalues,yRidgePred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training data (alpha=1) = \",np.mean(mse))\n",
    "print()\n",
    "\n",
    "ridReg=linear_model.Ridge(alpha=0.1)\n",
    "ridReg.fit(Xvalues,YregVal)\n",
    "yRidgePred=ridReg.predict(Xvalues)\n",
    "mse=cross_val_score(ridReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold CV (alpha=0.1) = \",np.mean(mse))\n",
    "mse=cross_val_score(ridReg,Xvalues,yRidgePred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training data (alpha=0.1) = \",np.mean(mse))\n",
    "print()\n",
    "\n",
    "ridReg=linear_model.Ridge(alpha=0.01)\n",
    "ridReg.fit(Xvalues,YregVal)\n",
    "yRidgePred=ridReg.predict(Xvalues)\n",
    "mse=cross_val_score(ridReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold CV (alpha=0.01) = \",np.mean(mse))\n",
    "mse=cross_val_score(ridReg,Xvalues,yRidgePred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training data (alpha=0.01) = \",np.mean(mse))\n",
    "print()\n",
    "\n",
    "ridReg=linear_model.Ridge(alpha=0.001)\n",
    "ridReg.fit(Xvalues,YregVal)\n",
    "yRidgePred=ridReg.predict(Xvalues)\n",
    "mse=cross_val_score(ridReg,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of 10-fold CV (alpha=0.001) = \",np.mean(mse))\n",
    "mse=cross_val_score(ridReg,Xvalues,yRidgePred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE of training data (alpha=0.001) = \",np.mean(mse))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b.\n",
    "\n",
    "(i) The estimated MSE under 10-fold CV for various alpha :\n",
    "\n",
    "Estimated MSE of 10-fold CV (alpha=10) =  -0.0200342497979\n",
    "Estimated MSE of 10-fold CV (alpha=1) =  -0.0197950213482\n",
    "Estimated MSE of 10-fold CV (alpha=0.1) =  -0.01991972494\n",
    "Estimated MSE of 10-fold CV (alpha=0.01) =  -0.0200559556275\n",
    "Estimated MSE of 10-fold CV (alpha=0.001) =  -0.020089527318\n",
    "\n",
    "(ii) The estimated MSE on the training data for various alpha :\n",
    "\n",
    "Estimated MSE of training data (alpha=10) =  -0.000152996873221\n",
    "Estimated MSE of training data (alpha=1) =  -8.73728120217e-05\n",
    "Estimated MSE of training data (alpha=0.1) =  -1.63387410814e-05\n",
    "Estimated MSE of training data (alpha=0.01) =  -1.31270726501e-06\n",
    "Estimated MSE of training data (alpha=0.001) =  -2.31024123202e-08\n",
    "\n",
    "(iii) Considering the MSE under both the cases, alpha=0.001 seems to be the best and the MSE is the least in that case.\n",
    "\n",
    "(iv) This shows that the amount of overfitting reduces the quality of the result, ie. the prediction of crime in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c. Using polynomical_features to do quadratic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MSE under 10-fold CV =  -0.129996952112\n",
      "Estimated MSE under training data =  -0.129996952112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly.fit_transform(Xvalues)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=2)),('linReg', linear_model.LinearRegression(fit_intercept=False))])\n",
    "model = model.fit(Xvalues, YregVal)\n",
    "\n",
    "\n",
    "yPolyPred=model.predict(Xvalues)\n",
    "msePoly=cross_val_score(model,Xvalues,YregVal,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE under 10-fold CV = \",np.mean(msePoly))\n",
    "\n",
    "msePoly=cross_val_score(model,Xvalues,yPolyPred,cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"Estimated MSE under training data = \",np.mean(msePoly))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c.\n",
    "\n",
    "(i) Estimated MSE under 10-fold CV =  -0.129996952112\n",
    "\n",
    "(ii) Estimated MSE under training data =  -0.129996952112\n",
    "\n",
    "(iii) This model hasn't reduced the MSE from the linear regression model. So, the quadratic model is not better than linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a. Using the full data set for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:  1994\n",
      "Dataset Shape::  (1994, 128)\n",
      "['True' 'True' 'True' 'False' 'False' 'False' 'True' 'True' 'True' 'False'\n",
      " 'True' 'True' 'True' 'False' 'False' 'False' 'True' 'False' 'True' 'True'\n",
      " 'True' 'True' 'False' 'False' 'False' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'True' 'True' 'True' 'True' 'True' 'True' 'False' 'True'\n",
      " 'True' 'True' 'True' 'False' 'False' 'True' 'True' 'True' 'False' 'False'\n",
      " 'False' 'False' 'True' 'True' 'True' 'True' 'True' 'True' 'False' 'False'\n",
      " 'True' 'True' 'False' 'False' 'True' 'True' 'True' 'False' 'True' 'True'\n",
      " 'True' 'False' 'True' 'True' 'True' 'False' 'False' 'True' 'True' 'False'\n",
      " 'False' 'False' 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'True'\n",
      " 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'False'\n",
      " 'False' 'True' 'False' 'False' 'True' 'True' 'False' 'True' 'False' 'True'\n",
      " 'True' 'False' 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'False' 'True' 'False' 'True' 'True' 'False' 'False' 'True' 'True'\n",
      " 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'True' 'False' 'True'\n",
      " 'True' 'True' 'False' 'False' 'True' 'False' 'True' 'True' 'True' 'True'\n",
      " 'False' 'True' 'False' 'True' 'True' 'True' 'True' 'False' 'False' 'True'\n",
      " 'False' 'True' 'True' 'True' 'False' 'True' 'False' 'False' 'True' 'True'\n",
      " 'True' 'True' 'True' 'True' 'True' 'False' 'False' 'True' 'False' 'True'\n",
      " 'True' 'True' 'True' 'True' 'True' 'False' 'True' 'False' 'True' 'False'\n",
      " 'False' 'True' 'True' 'True' 'True' 'False' 'False' 'True' 'False' 'False'\n",
      " 'True' 'True' 'False' 'False' 'True' 'True' 'False' 'True' 'False' 'False'\n",
      " 'True' 'False' 'True' 'True' 'True' 'True' 'True' 'True' 'True' 'False'\n",
      " 'True' 'True' 'False' 'True' 'True' 'False' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'True' 'True' 'True' 'False' 'False' 'False' 'False' 'True'\n",
      " 'False' 'True' 'False' 'False' 'True' 'False' 'True' 'False' 'False'\n",
      " 'False' 'True' 'False' 'False' 'True' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'True' 'True' 'False' 'False' 'False' 'True' 'True' 'True'\n",
      " 'True' 'False' 'False' 'False' 'False' 'True' 'True' 'True' 'False' 'True'\n",
      " 'False' 'True' 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'True'\n",
      " 'True' 'True' 'True' 'False' 'True' 'False' 'True' 'True' 'True' 'False'\n",
      " 'False' 'True' 'True' 'True' 'True' 'False' 'False' 'False' 'True' 'True'\n",
      " 'True' 'True' 'False' 'True' 'True' 'False' 'True' 'False' 'False' 'False'\n",
      " 'True' 'True' 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'True'\n",
      " 'False' 'True' 'True' 'False' 'True' 'False' 'True' 'False' 'True' 'False'\n",
      " 'False' 'True' 'True' 'True' 'False' 'True' 'True' 'False' 'True' 'True'\n",
      " 'True' 'True' 'True' 'False' 'True' 'False' 'False' 'True' 'True' 'True'\n",
      " 'False' 'True' 'False' 'True' 'False' 'True' 'False' 'False' 'True' 'True'\n",
      " 'True' 'True' 'False' 'False' 'True' 'False' 'True' 'False' 'False' 'True'\n",
      " 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'False' 'True' 'False'\n",
      " 'False' 'True' 'True' 'False' 'True' 'True' 'True' 'True' 'True' 'True'\n",
      " 'False' 'True' 'False' 'True' 'True' 'True' 'True' 'False' 'False' 'True'\n",
      " 'True' 'False' 'True' 'True' 'True' 'False' 'False' 'False' 'False' 'True'\n",
      " 'True' 'False' 'True' 'False' 'True' 'False' 'True' 'True' 'True' 'True'\n",
      " 'True' 'True' 'True' 'False' 'True' 'False' 'True' 'True' 'False' 'True'\n",
      " 'False' 'False' 'False' 'False' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'False' 'True' 'False' 'True' 'True' 'True' 'False' 'False'\n",
      " 'False' 'False' 'False' 'True' 'True' 'False' 'False' 'True' 'True'\n",
      " 'False' 'True' 'True' 'False' 'True' 'False' 'False' 'True' 'True' 'True'\n",
      " 'True' 'True' 'False' 'True' 'False' 'False' 'True' 'False' 'True' 'True'\n",
      " 'True' 'True' 'False' 'True' 'True' 'False' 'True' 'False' 'False' 'False'\n",
      " 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'True' 'True' 'False'\n",
      " 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'True' 'True' 'False'\n",
      " 'False' 'True' 'False' 'False' 'True' 'True' 'True' 'False' 'True' 'True'\n",
      " 'True' 'False' 'False' 'False' 'True' 'False' 'True' 'True' 'True' 'True'\n",
      " 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'True' 'False' 'False'\n",
      " 'False' 'True' 'False' 'True' 'True' 'True' 'False' 'False' 'False'\n",
      " 'False' 'True' 'True' 'True' 'True' 'True' 'False' 'False' 'True' 'True'\n",
      " 'True' 'False' 'True' 'True' 'True' 'False' 'True' 'False' 'True' 'True'\n",
      " 'True' 'False' 'True' 'True' 'True' 'True' 'False' 'True' 'True' 'True'\n",
      " 'True' 'True' 'False']\n",
      "Accuracy =  100.0 %\n",
      "Precision =  100.0 %\n",
      "Recall =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.preprocessing import Imputer\n",
    "reader=csv.reader(open('C:/CrimePredictionData/communities-crime-full.csv','r'))\n",
    "writer=csv.writer(open('C:/CrimePredictionData/outputSampleFull.csv','w',newline=''))\n",
    "i=0\n",
    "for row in reader:\n",
    "    for i in range(len(row)):\n",
    "        if(str(row[i])=='?'):\n",
    "            row[i]=0\n",
    "    if(i==0):\n",
    "        row.append(\"highCrime\")\n",
    "        i=1\n",
    "    if(row[127]!='ViolentCrimesPerPop'):\n",
    "        if(float(row[127])>0.1):\n",
    "            row.append(\"true\")\n",
    "        else:\n",
    "            row.append(\"false\")\n",
    "    writer.writerow(row)\n",
    "\n",
    "\n",
    "data=pd.read_csv('C:/CrimePredictionData/outputSampleFull.csv',header=0)\n",
    "\n",
    "print (\"Dataset Length: \", len(data))\n",
    "print (\"Dataset Shape:: \", data.shape)\n",
    "\n",
    "Xvalues=data.values[:,5:127]\n",
    "Yvalues=data.values[:,127]\n",
    "\n",
    "for i in range(len(Yvalues)):\n",
    "    Yvalues[i]=str(Yvalues[i])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xvalues,Yvalues,test_size = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_training=list()\n",
    "for val in Y_train:\n",
    "    Y_training.append(val)\n",
    "\n",
    "#X_train=np.array(X_train).reshape(-1,1)\n",
    "#Y_training=Y_training.reshape(-1,1)\n",
    "y_training=np.asarray(Y_training)\n",
    "y_training=y_training.reshape(-1,1)\n",
    "dTreeClass=dTreeClass.fit(X_train,y_training)\n",
    "\n",
    "#X_test=np.array(X_test).reshape((-1,1))\n",
    "y_pred=dTreeClass.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "print(\"Accuracy = \",accuracy_score(Y_test,y_pred)*100,\"%\")\n",
    "print(\"Precision = \",precision_score(Y_test,y_pred,pos_label=\"True\")*100,\"%\")\n",
    "print(\"Recall = \",recall_score(Y_test,y_pred,pos_label=\"True\")*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a.\n",
    "\n",
    "The CV results are worse, since the values are missing. This shows that missing values make it complex for the model to learn and predict."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
